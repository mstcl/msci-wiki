---
title: Week 6
date: 2023-11-08
author: NP
jupyter: python3
execute:
  cache: true
---

::: {.callout-note appearance="simple"}
Part of this week was the school of arts' reading week so I only set out to do
a few things.
:::

## Tasks

- Some chores
  - [x] Update the plot on [Week 4](./week_4.qmd#plotting-snapshots) with axis labels.
  - [x] Update PEP's docs. See
    [README.md](https://github.com/dlactivematter/persistent-exclusion-process/blob/main/README.md).
  - [x] Regenerate dataset with new orientation fix.
- Continue with data analysis.
  - [x] Demo of mean orientation over time.
  - [x] Demo of cluster labelling and size distribution.

## Summary

### A problem with how the h5 binaries were saved

While doing this, I found out that the matrix is being saved with the
background at 0, and the 4 orientation are 0, 1, 2, 3. This means the 0^th^
orientation is confused with the background. To fix this, I modified the
`image()` method of the `Lattice` object (in `lattice.py`) to add 1 to the
orientation: so they start at 1 (to 4) instead:

```{.python filename="lattice.py"}
def image(self) -> np.ndarray:
    """Define the (x,y) lattice array

    :param self: object's attributes
    :returns: the lattice array [np.ndarray]

    ------

    # Explanation

    Create a matrix of physical size [`n_x`, `n_y`] and fill it as follows:

    * get the coordinates of all particles with positions()
    * set those coordinates to the orientation

    """
    matrix = np.zeros((self.n_x, self.n_y))
    x_pos, y_pos = self.positions()
    matrix[x_pos, y_pos] = self.orientation + 1
    return matrix
```

Thus, I had to regenerate all the dataset on BluePebble. Snippet to batch run all
SLURM scripts in a directory (I had a separate script for each density):

```{.bash code-line-numbers="false"}
for file in *.sbat; do sbatch $file; done
```

### Steady-state in orientation

Here, I plotted the average orientation of a snapshot, plotting its accumulated
mean over time, for this particular dataset, $\alpha = 0.23$ and $\phi = 0.25$.

This first part of the code is part of `utils.py`;

```{python}
import re

import h5py
import numpy as np


def get_ds_iters(key_list: list) -> list:
    """
    Get all the unique iteration numbers

    :param key_list: a list of all the possible dataset keys/names

    :returns: a list of unique iterations
    """
    iter_n = []
    for val in key_list:
        if re.search("^conf_\d+$", val):
            iter_n.append(int(val[5:]))
    return sorted(iter_n)


def get_mean_orientation(file) -> list:
    """
    Get the mean orientation at each iteration

    :param file: the h5 file to open [str]
    :returns: mean orientation of length 1000 [list]
    """
    hf = h5py.File(file, "r")
    key_list = list(hf.keys())
    iter_n = get_ds_iters(key_list)
    ori = []
    ori_acm = []
    for idx, val in enumerate(iter_n):
        sshot = np.array(hf[f"conf_{val}"]).flatten()
        avg_ori = np.average(sshot[np.where(sshot != 0)[0]] - 1)
        ori.append(avg_ori)
        ori_acm.append(np.mean(ori))
    return ori_acm
```

We now plot using those two functions:

```{python}
import matplotlib.pyplot as plt
from cmcrameri import cm

def plot_mean_ori(file, let):
    cmap = plt.get_cmap(name='cmc.bilbaoS', lut=5)
    hf = h5py.File(file, "r")
    fig, (ax1, ax2) = plt.subplots(
        1,
        2,
        figsize=(9, 3),
        width_ratios = [1,1.25],
        constrained_layout=True
    )
    iters = get_ds_iters(hf.keys())
    fig.colorbar(plt.cm.ScalarMappable(cmap=cmap), ax=ax1)
    img = hf[f"conf_{iters[-1]}"]
    ax1.matshow(img, cmap=cmap)
    ax2.set_xlabel(r"Number of iterations, $N$")
    ax2.set_ylabel("Average orientation")
    ax2.plot(
        iters,
        get_mean_orientation(file),
        c="k",
    )
    fig.suptitle(
        r"({}) $\alpha = {}$".format(let, file[25:30]),
        x=0.15,
        ha="left",
        fontweight="bold"
    )
    ax2.grid(alpha=.3)
    ax2.set_axisbelow(True)
    plt.show()
```

```{python}
plot_mean_ori('../assets/dataset_tumble_0.023_0.25.h5', "a")
plot_mean_ori('../assets/dataset_tumble_0.500_0.25.h5', "b")
```

I think it's worth rerunning this analysis where the number of iterations is the
same in both (so h5 size and length takes the form of the one with the most
iterations, $\alpha = 0.023$ in this case), just so the x-scale matches.

### Cluster analysis

Using sample code from [this notebook](../shared/ftnotes/label.ipynb).

```{python}
from scipy import ndimage

def label_cluster(file, let):
    cmap = plt.get_cmap(name='cmc.bilbaoS', lut=5)
    cmap_label = plt.get_cmap(name='cmc.actonS')
    hf = h5py.File(file, "r")
    fig, (ax1, ax2, ax3) = plt.subplots(
        1,
        3,
        figsize=(10, 3),
        width_ratios=(1,1,1.3),
        constrained_layout=True
    )
    iters = get_ds_iters(hf.keys())
    fig.suptitle(
        r"({}) $\alpha = {}$".format(let, file[25:30]),
        x=0.1,
        ha="left",
        fontweight="bold"
    )

    img = hf[f"conf_{iters[-1]}"]
    ax1.matshow(img, cmap=cmap)

    kernel = [[0, 1, 0], [1, 1, 1], [0, 1, 0]]
    labelled, nlabels = ndimage.label(img, structure=kernel)
    ax2.matshow(labelled, cmap=cmap_label)

    cluster_sizes = np.bincount(labelled.flatten())[1:]
    min_c = cluster_sizes.min()
    max_c = cluster_sizes.max()
    # one can also make bins in log space, in Soto (2014) the base range is 1-1.5
    # bin_edges = np.logspace(np.log2(min_c), np.log2(max_c), 30, base=2)
    bin_edges = np.linspace(min_c, max_c, 100)
    counts, _ = np.histogram(cluster_sizes, bins=bin_edges, density=True)
    ax3.grid(alpha=.3)
    ax3.set_axisbelow(True)
    ax3.scatter(bin_edges[:-1], counts, edgecolor=(0,0,0,1), facecolor=(0,0,0,.3))
    ax3.set_yscale("log"), ax3.set_xscale("log")
    ax3.set_xlabel("Cluster size")

    fig.colorbar(plt.cm.ScalarMappable(cmap=cmap), ax=ax1)
    # fig.colorbar(plt.cm.ScalarMappable(cmap=cmap_label), ax=ax2)
    plt.show()
```

```{python}
label_cluster('../assets/dataset_tumble_0.023_0.25.h5', "a")
label_cluster('../assets/dataset_tumble_0.500_0.25.h5', "b")
```

### Cluster size distribution grid

Same values [this one](./week_4.qmd#plotting-snapshots), but with the above
cluster size analysis for the last iteration of each dataset

![](../assets/csize.png)

In addition, we can also look at how the number of cluster varies.
